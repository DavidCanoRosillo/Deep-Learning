{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pylab inline\n!pip install natsort\n\nimport torch\nimport torchvision\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom zipfile import ZipFile\nfrom natsort import natsorted\nfrom PIL import Image\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\ntorch.manual_seed(1)\n!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-03-24T19:10:00.566329Z","iopub.execute_input":"2022-03-24T19:10:00.566907Z","iopub.status.idle":"2022-03-24T19:10:03.922616Z","shell.execute_reply.started":"2022-03-24T19:10:00.566867Z","shell.execute_reply":"2022-03-24T19:10:03.921738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating custom dataset to load images","metadata":{}},{"cell_type":"code","source":"class segments2facades(torch.utils.data.Dataset):\n    def __init__(self, main_dir, transform, split=False):\n        self.main_dir = main_dir\n        self.transform = transform\n        all_imgs = os.listdir(main_dir)\n        self.total_imgs = natsorted(all_imgs)\n        self.split = split\n\n    def __len__(self):\n        return len(self.total_imgs)\n           \n    def _separate(self, img):\n        img = np.array(img, dtype=np.uint8)\n        h, w, _ = img.shape\n        w = int(w/2)\n        return Image.fromarray(img[:, w:, :]), Image.fromarray(img[:, :w, :])\n    \n    def __getitem__(self, idx):\n        img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n        image = Image.open(img_loc).convert(\"RGB\")\n        if (not self.split):\n            return self.transform(image)\n        input_t, output_t = self._separate(image)\n        input_t, output_t = self.transform(input_t), self.transform(output_t)\n        return input_t,output_t","metadata":{"execution":{"iopub.status.busy":"2022-03-24T19:13:22.787836Z","iopub.execute_input":"2022-03-24T19:13:22.78809Z","iopub.status.idle":"2022-03-24T19:13:22.79737Z","shell.execute_reply.started":"2022-03-24T19:13:22.788061Z","shell.execute_reply":"2022-03-24T19:13:22.796387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BS = 1\nLR = 1e-4\nepochs = 20","metadata":{"execution":{"iopub.status.busy":"2022-03-24T19:13:25.34036Z","iopub.execute_input":"2022-03-24T19:13:25.34087Z","iopub.status.idle":"2022-03-24T19:13:25.346435Z","shell.execute_reply.started":"2022-03-24T19:13:25.340827Z","shell.execute_reply":"2022-03-24T19:13:25.344271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"std_transform = torchvision.transforms.Compose([\n    torchvision.transforms.Resize((128, 128)),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\nfacades_ds = segments2facades('../input/pix2pix-dataset/cityscapes/cityscapes/train', transform=std_transform, split=True)\nfacades_loader = torch.utils.data.DataLoader(facades_ds, batch_size=BS, shuffle = True, drop_last = True)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T19:13:26.26849Z","iopub.execute_input":"2022-03-24T19:13:26.268906Z","iopub.status.idle":"2022-03-24T19:13:26.304667Z","shell.execute_reply.started":"2022-03-24T19:13:26.268871Z","shell.execute_reply":"2022-03-24T19:13:26.304043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def concat_img(imgs):\n    figsize(16,16)\n    figure()\n    imgs = (imgs + 1) / 2\n    imgs = imgs.movedim((0, 1, 2, 3), (0, 3, 1, 2)).detach().cpu().numpy() \n    axs = imshow(np.concatenate(imgs.tolist(), axis=1))\n    plt.axis('off')\n    plt.show()\n    \ndef print_img(content, style, output):\n    printable = torch.cat((content.cpu(), style.cpu(), output.cpu()), 0)\n    concat_img((printable).detach().cpu())\n    \nsegment, real = next(iter(facades_loader))\nprint(\"Segments vs real\")\nconcat_img(torch.cat((segment[:1], real[:1]), 0))","metadata":{"execution":{"iopub.status.busy":"2022-03-24T19:13:27.004505Z","iopub.execute_input":"2022-03-24T19:13:27.005112Z","iopub.status.idle":"2022-03-24T19:13:27.222931Z","shell.execute_reply.started":"2022-03-24T19:13:27.00508Z","shell.execute_reply":"2022-03-24T19:13:27.222255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining model architectures","metadata":{}},{"cell_type":"code","source":"# pix2pix is an image translation model which takes x image and translates it to a y_hat image.\n# ej: segments to facade\n\n# CGAN loss: E[log(D(x, y))] + E[log(1-D(x, G(x))]\n# Where discriminator wants to maximize the function and generator wants to minimize it\n\n# L1 loss:   E[||y - G(x)||]\n# L1 loss rather than L2 eucledian distance to disencourage blurriness.\n\n# Total loss: CGAN_loss + lambda * L1_loss\n# To note is the absence of a z noise input for the generator, instead they use dropout at test and train time.\n\n# All blocks are as follows:\n# [Conv, Batchnorm, Relu]\n# with batchnorm applied both at train and test time\n\n# Generator:\n# g: x -> y\n# It is a U-Net encoder decoder structure\n# First half:\n# Ln(Ln-1(x))\n# Second half:\n# Ln(cat[Ln-1(x), Ln - n/2 - 1()])\n\nclass Generator(torch.nn.Module):\n    def __init__(self, d):\n        def block_down(in_c, out_c, kernel, stride, padding, p, batchnorm=True):\n            if (not batchnorm):\n                return [torch.nn.Conv2d(in_c, out_c, kernel, stride, padding, bias=False),\n                    torch.nn.LeakyReLU(0.2),\n                    torch.nn.Dropout(p),\n                    ] # Batchnorm and Dropout regardless train or test\n            return [torch.nn.Conv2d(in_c, out_c, kernel, stride, padding, bias=False),\n                    torch.nn.BatchNorm2d(out_c),\n                    torch.nn.LeakyReLU(0.2),\n                    torch.nn.Dropout(p),\n                    ] # Batchnorm and Dropout regardless train or test\n        \n        def block_up(in_c, out_c, kernel, stride, padding, p, last=False):\n            if (last):\n                return [torch.nn.ConvTranspose2d(in_c, out_c, kernel, stride, padding, bias=False),\n                        torch.nn.BatchNorm2d(out_c),\n                        torch.nn.Tanh(),\n                        ] # Batchnorm and Dropout regardless train or test\n            return [torch.nn.ConvTranspose2d(in_c, out_c, kernel, stride, padding, bias=False),\n                    torch.nn.BatchNorm2d(out_c),\n                    torch.nn.ReLU(0.2),\n                    torch.nn.Dropout(p),\n                   ]\n\n        super(Generator, self).__init__()\n        self.down = torch.nn.Sequential(\n            *block_down(3, d, 4, 2, 1, 0, batchnorm=False), #0\n            *block_down(d, d * 2, 4, 2, 1, 0.5, batchnorm=False), #1\n            *block_down(d * 2, d * 4, 4, 2, 1, 0.5), #2\n            *block_down(d * 4, d * 8, 4, 2, 1, 0.5), #3\n            *block_down(d * 8, d * 8, 4, 2, 1, 0.5), #4\n            # ------\n            #*block_down(d * 8, d * 8, 4, 2, 1, 0), #5\n            #*block_down(d * 8, d * 8, 4, 2, 1, 0), #6\n            #*block_down(d * 8, d * 8, 4, 2, 1, 0), #7\n        )\n        \n        self.intermediate = torch.nn.Sequential(\n            *block_down(d * 8, d * 8, 4, 2, 1, 0.5),\n            *block_up(d * 8, d * 8, 4, 2, 1, 0.5),\n        )\n        \n        self.up = torch.nn.Sequential(\n            #*block_up(d * 8, d * 8, 4, 2, 1, 0), # 7\n            #*block_up(d * 8 * 2, d * 8, 4, 2, 1, 0), # 6\n            #*block_up(d * 8 * 2, d * 8, 4, 2, 1, 0), # 5\n            *block_up(d * 8 * 2, d * 8, 4, 2, 1, 0.5), # 4\n            *block_up(d * 8 * 2, d * 4, 4, 2, 1, 0.5), # 3 \n            *block_up(d * 4 * 2, d * 2, 4, 2, 1, 0), # 2\n            *block_up(d * 2 * 2, d, 4, 2, 1, 0), # 1\n            #*block_up(d * 2, d, 4, 2, 1, 0), # 0\n        )\n            \n        self.last = torch.nn.Sequential(\n            *block_up(d, 3, 4, 2, 1, 0, last=True)\n        )\n    def forward(self, x):\n        outputs_down = []\n        i = 0\n        for layer in self.down:\n            x = layer(x)\n            if isinstance(layer, torch.nn.LeakyReLU):\n                outputs_down.append(x)\n        \n        x = self.intermediate(x)\n\n        for layer in self.up:\n            if isinstance(layer, torch.nn.ConvTranspose2d):\n                x = layer(torch.cat((x, outputs_down[len(outputs_down) - 1 - i]), 1))\n                i += 1\n            else:\n                x = layer(x)\n        x = self.last(x)\n        return x\n\nG = Generator(64).cuda()\nG(segment.cuda()).shape","metadata":{"execution":{"iopub.status.busy":"2022-03-24T19:13:27.601778Z","iopub.execute_input":"2022-03-24T19:13:27.602101Z","iopub.status.idle":"2022-03-24T19:13:27.855319Z","shell.execute_reply.started":"2022-03-24T19:13:27.602068Z","shell.execute_reply":"2022-03-24T19:13:27.854557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Discriminator takes segments and facade, facade can be generated or real\n# To note is that the output is not a prediction for the whole image but for \n# different parts of the image (search PATCHGAN)\nclass Discriminator(torch.nn.Module):\n    def __init__(self, d):\n        super(Discriminator, self).__init__()\n        def block_conv(in_c, out_c, kernel, stride, padding, batchnorm=True):\n            if(batchnorm):\n                return [torch.nn.Conv2d(in_c, out_c, kernel, stride, padding),\n                        torch.nn.BatchNorm2d(out_c),\n                        torch.nn.LeakyReLU(0.2),\n                        ]\n            return [torch.nn.Conv2d(in_c, out_c, kernel, stride, padding),\n                   torch.nn.LeakyReLU(0.2)]\n        \n        self.convos = torch.nn.Sequential(\n            *block_conv(6, d, 4, 2, 1, batchnorm=False),\n            *block_conv(d, d * 2, 4, 2, 1),\n            *block_conv(d * 2, d * 4, 4, 2, 1),\n            *block_conv(d * 4, d * 8, 4, 1, 1),\n            torch.nn.Conv2d(d * 8, 1, 4, 1, 1),\n            torch.nn.Sigmoid(),\n        )\n    def forward(self, x, y):\n        tomonkey = torch.cat((x, y), 1)\n        tomonkey = self.convos(tomonkey)\n        return tomonkey\n\n#D = Discriminator(64).cuda()\n#D(segment, real).shape\n#d_loss = BinaryCrossEntropy(D([y_hat, y]), [0, 1])\n#_loss = BinaryCrossEntropy(D[y_hat], [1]) + alpha * torch.mean(||y - y_hat||1)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T19:27:21.417229Z","iopub.execute_input":"2022-03-24T19:27:21.41778Z","iopub.status.idle":"2022-03-24T19:27:21.426845Z","shell.execute_reply.started":"2022-03-24T19:27:21.41774Z","shell.execute_reply":"2022-03-24T19:27:21.426147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpha = 100\nLR = 0.0002\n\nG = Generator(64).cuda()\nD = Discriminator(128).cuda()\n\nepochs = 30\ng_lr = LR\nd_lr = LR\ntrue_label = 0.9\nfake_label = 0\n\ng_optim = torch.optim.Adam(G.parameters(), lr=g_lr, betas = (0.5, 0.999))\nd_optim = torch.optim.Adam(D.parameters(), lr=d_lr, betas = (0.5, 0.999))\n\n# Commented code for loading models\n\"\"\"\nPATH = '../input/model-v1/pix2pix_gen_shoe_v1'\ncheckpoint = torch.load(PATH)\nG.load_state_dict(checkpoint['model_state_dict'])\ng_optim.load_state_dict(checkpoint['optimizer_state_dict'])\niters = checkpoint['iters']\n\nPATH = '../input/model-v1/pix2pix_disc_shoe_v1'\ncheckpoint = torch.load(PATH)\nD.load_state_dict(checkpoint['model_state_dict'])\nd_optim.load_state_dict(checkpoint['optimizer_state_dict'])\niters = checkpoint['iters']\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-03-24T19:27:26.277754Z","iopub.execute_input":"2022-03-24T19:27:26.278279Z","iopub.status.idle":"2022-03-24T19:27:26.589737Z","shell.execute_reply.started":"2022-03-24T19:27:26.27824Z","shell.execute_reply":"2022-03-24T19:27:26.589081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def discriminator_loss(inputs, targets):\n    BCE = torch.nn.BCELoss().cuda()\n    return BCE(inputs, targets)\n\ndef generator_loss(d_fake_predictions, generated, real, alpha):\n    BCE = torch.nn.BCELoss().cuda()\n    L1 = torch.nn.L1Loss().cuda()\n    gan_loss = BCE(d_fake_predictions, torch.ones(d_fake_predictions.size()).cuda())\n    # l1 loss measures distance between real and generated image\n    l1_loss = L1(generated, real)\n    return gan_loss + alpha * l1_loss","metadata":{"execution":{"iopub.status.busy":"2022-03-24T19:27:27.767911Z","iopub.execute_input":"2022-03-24T19:27:27.768667Z","iopub.status.idle":"2022-03-24T19:27:27.774915Z","shell.execute_reply.started":"2022-03-24T19:27:27.768621Z","shell.execute_reply":"2022-03-24T19:27:27.774197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def adjust_learning_rate(optimiser, iters, learning_rate_decay):\n    for param_group in optimiser.param_groups:\n        param_group['lr'] = LR / (1.0 + learning_rate_decay * iters)\nlearning_rate_decay=5e-5","metadata":{"execution":{"iopub.status.busy":"2022-03-24T19:27:28.970868Z","iopub.execute_input":"2022-03-24T19:27:28.971491Z","iopub.status.idle":"2022-03-24T19:27:28.981564Z","shell.execute_reply.started":"2022-03-24T19:27:28.971442Z","shell.execute_reply":"2022-03-24T19:27:28.980536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the models, final results at the end of file","metadata":{}},{"cell_type":"code","source":"#torch.autograd.set_detect_anomaly(True)\niters = 0\nfor epoch in range(epochs):\n    G.train()\n    D.train()\n    for batch_idx, (segment, real) in enumerate(facades_loader):\n        segment, real = segment.cuda(), real.cuda()\n        \n        # Train critic:\n        d_optim.zero_grad()\n        \n        fake = G(segment)\n        d_real = D(segment, real)\n        d_fake = D(segment, fake)\n        predictions = torch.cat((d_real, d_fake), 0)\n        targets = torch.cat((torch.ones(d_real.size()).cuda(), torch.zeros(d_fake.size()).cuda()), 0)\n        \n        d_loss = discriminator_loss(predictions, targets)\n            \n        d_loss.backward()\n        d_optim.step()\n\n        # Train generator:\n        g_optim.zero_grad()\n        \n        fake = G(segment)\n        d_fake = D(segment, fake)\n        g_loss = generator_loss(d_fake, fake, real, alpha)\n\n        g_loss.backward()\n        g_optim.step()\n        if (batch_idx % 100 == 0):\n            print('Epoch {} batch {} Discriminator loss: {:.3f} Generator loss: {:.3f}'.format(epoch, batch_idx, d_loss, g_loss))\n        if (batch_idx % 1000 == 0):\n            print_img(segment[:1], fake[:1], real[:1])\n            plt.show()\n        iters += 1","metadata":{"execution":{"iopub.status.busy":"2022-03-24T19:27:29.458039Z","iopub.execute_input":"2022-03-24T19:27:29.458507Z","iopub.status.idle":"2022-03-24T20:54:03.035955Z","shell.execute_reply.started":"2022-03-24T19:27:29.458471Z","shell.execute_reply":"2022-03-24T20:54:03.03519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading validation data to test if it generalizes to unseen images\n\nval_ds = segments2facades('../input/pix2pix-dataset/cityscapes/cityscapes/val', transform=std_transform, split=True)\nval_loader = torch.utils.data.DataLoader(facades_ds, batch_size=BS, shuffle = True, drop_last = True)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T20:57:14.403894Z","iopub.execute_input":"2022-03-24T20:57:14.404179Z","iopub.status.idle":"2022-03-24T20:57:14.420802Z","shell.execute_reply.started":"2022-03-24T20:57:14.404148Z","shell.execute_reply":"2022-03-24T20:57:14.420066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing the model on validation data","metadata":{}},{"cell_type":"code","source":"# We can see the generator completely ignores lighting as the segmentation\n# does not provide information on it. Discriminator is probably taking \n# advantage of this. It would be interesting to see if further training makes\n# the generator learn to add lighting.\nsegment, real = next(iter(val_loader))\nsegment, real = segment.cuda(), real.cuda()\nfake = G(segment)\nsample = 1\nfor i in range(7):\n    print_img(segment[sample - 1:sample], fake[sample - 1:sample], real[sample - 1:sample])\n    plt.show()\n    sample += 1","metadata":{"execution":{"iopub.status.busy":"2022-03-24T20:58:31.123623Z","iopub.execute_input":"2022-03-24T20:58:31.124314Z","iopub.status.idle":"2022-03-24T20:58:38.521872Z","shell.execute_reply.started":"2022-03-24T20:58:31.124275Z","shell.execute_reply":"2022-03-24T20:58:38.521244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_models():\n    torch.save({\n            'epoch': epoch,\n            'model_state_dict': G.state_dict(),\n            'optimizer_state_dict': g_optim.state_dict(),\n            'loss': g_loss,\n            'iters': iters,\n            }, './segment2street_gen_v1')\n    torch.save({\n            'epoch': epoch,\n            'model_state_dict': D.state_dict(),\n            'optimizer_state_dict': d_optim.state_dict(),\n            'loss': d_loss,\n            'iters': iters,\n            }, './segment2street_disc_v1')\nsave_models()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T21:13:03.374134Z","iopub.execute_input":"2022-03-24T21:13:03.374379Z","iopub.status.idle":"2022-03-24T21:13:04.098482Z","shell.execute_reply.started":"2022-03-24T21:13:03.374352Z","shell.execute_reply":"2022-03-24T21:13:04.097692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Thanks for reading :P","metadata":{}}]}